{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e73239b-7c58-443c-825e-f968059e0a53",
   "metadata": {},
   "source": [
    "# Predictive Analytics Homework 3 – Regression Modeling & Evaluation\n",
    "*(Submitted by - Mayank Singh)*\n",
    "\n",
    "This exercise involves building, tuning, and evaluating multiple **regression models** to predict customer spending. It walks through the complete ML pipeline: from data preprocessing and model training to evaluation using cross-validation and final holdout testing.\n",
    "\n",
    "## Dataset\n",
    "The dataset includes customer-level features. This dataset contains data about whether or not different consumers made a purchase in response to a test mailing of a certain catalog and, in case of a purchase, how much money each consumer spent. The objective is to predict the **`Purchase`** amount.\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. **Build numeric prediction models** that predict Spending based on the other available customer informationn.\n",
    "1. **Compare multiple regression models** using RMSE as the evaluation metric.\n",
    "3. **Tune and finalize** the best-performing model using Nested Cross-Validation.\n",
    "4. **Interpret** final performance using RMSE on a holdout test set.\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Breakdown\n",
    "\n",
    "### 1. Loading Data\n",
    "- Load and preprocess the dataset.\n",
    "\n",
    "### 2. Model Comparison\n",
    "- Define and evaluate multiple regression models:\n",
    "  - `LinearRegression`\n",
    "  - `KNeighborsRegressor`\n",
    "  - `DecisionTreeRegressor`\n",
    "  - `SVR`\n",
    "  - `RandomForestRegressor`\n",
    "  - `GradientBoostingRegressor`\n",
    "  - `XGBRegressor`\n",
    "  - `LGBMRegressor`\n",
    "  - `MLPRegressor`\n",
    "- Use **Nested Cross-Validation** with preprocessing (standard scaling).\n",
    "- Compare based on **mean RMSE and standard deviation** across folds.\n",
    "\n",
    "### 3. Final Model Selection & Evaluation\n",
    "- Select the **best model** (e.g., `XGBoost` or `NeuralNet`) based on CV results.\n",
    "- Perform **hyperparameter tuning** using `GridSearchCV`.\n",
    "- Refit the model on the full training data.\n",
    "- Evaluate on the **holdout test set**.\n",
    "- Report:\n",
    "  - Final RMSE\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation Metric\n",
    "- **RMSE (Root Mean Squared Error)**: Used consistently for cross-validation and final evaluation.\n",
    "- Lower RMSE indicates better model performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Outcome\n",
    "The notebook concludes with the **best-tuned model** evaluated on the holdout set.\n",
    "\n",
    "---\n",
    "\n",
    "> Built with: Python, `scikit-learn`, `XGBoost`, `LightGBM`, `Pandas`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f91671-4732-4629-bfc3-0a6b539912d0",
   "metadata": {},
   "source": [
    "# Part A - Modeling on All Customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056f38f1-989d-493a-b97e-e8599d77b124",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e13ba280-ed9f-4bac-9401-7c94a840787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"lightgbm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52ca362-d7cc-461a-8d6e-b8a6c079ff42",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "The dataset `hw3data.csv` is loaded into a pandas DataFrame. From this, three columns — `sequence_number`, `Spending`, and `Purchase` — are removed to isolate the predictors into `X_full`. The target variable `y_full` is defined separately using the `Spending` column. This setup ensures that only relevant features are used for training, while the target is clearly separated for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02fac528-73c0-4f29-ae20-f88618a0c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 2: Load Data ===\n",
    "df = pd.read_csv(\"hw3data.csv\")\n",
    "X_full = df.drop(columns=[\"sequence_number\", \"Spending\", \"Purchase\"])\n",
    "y_full = df[\"Spending\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "030c6d4b-2223-4c08-bd08-2a4d55a2045a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>US</th>\n",
       "      <th>source_a</th>\n",
       "      <th>source_c</th>\n",
       "      <th>source_b</th>\n",
       "      <th>source_d</th>\n",
       "      <th>source_e</th>\n",
       "      <th>source_m</th>\n",
       "      <th>source_o</th>\n",
       "      <th>source_h</th>\n",
       "      <th>source_r</th>\n",
       "      <th>...</th>\n",
       "      <th>source_u</th>\n",
       "      <th>source_p</th>\n",
       "      <th>source_x</th>\n",
       "      <th>source_w</th>\n",
       "      <th>Freq</th>\n",
       "      <th>last_update_days_ago</th>\n",
       "      <th>1st_update_days_ago</th>\n",
       "      <th>Web order</th>\n",
       "      <th>Gender=male</th>\n",
       "      <th>Address_is_res</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3662</td>\n",
       "      <td>3662</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2900</td>\n",
       "      <td>2900</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3883</td>\n",
       "      <td>3914</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>829</td>\n",
       "      <td>829</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>869</td>\n",
       "      <td>869</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   US  source_a  source_c  source_b  source_d  source_e  source_m  source_o  \\\n",
       "0   1         0         0         1         0         0         0         0   \n",
       "1   1         0         0         0         0         1         0         0   \n",
       "2   1         0         0         0         0         0         0         0   \n",
       "3   1         0         1         0         0         0         0         0   \n",
       "4   1         0         1         0         0         0         0         0   \n",
       "\n",
       "   source_h  source_r  ...  source_u  source_p  source_x  source_w  Freq  \\\n",
       "0         0         0  ...         0         0         0         0     2   \n",
       "1         0         0  ...         0         0         0         0     0   \n",
       "2         0         0  ...         0         0         0         0     2   \n",
       "3         0         0  ...         0         0         0         0     1   \n",
       "4         0         0  ...         0         0         0         0     1   \n",
       "\n",
       "   last_update_days_ago  1st_update_days_ago  Web order  Gender=male  \\\n",
       "0                  3662                 3662          1            0   \n",
       "1                  2900                 2900          1            1   \n",
       "2                  3883                 3914          0            0   \n",
       "3                   829                  829          0            1   \n",
       "4                   869                  869          0            0   \n",
       "\n",
       "   Address_is_res  \n",
       "0               1  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f51858d-2022-4a74-bf69-041b8bf54e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    127.87\n",
       "1      0.00\n",
       "2    127.48\n",
       "3      0.00\n",
       "4      0.00\n",
       "Name: Spending, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93e972f-9d6c-4c74-a18f-e8f5fb1a58e2",
   "metadata": {},
   "source": [
    "## Holdout Split\n",
    "\n",
    "The dataset is then split into training and holdout sets using an 80-20 split. This means 80% of the data will be used to train and validate models through cross-validation, while 20% will be reserved as an untouched set to evaluate final model performance. A fixed random seed ensures the results are reproducible across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b32bef8-36f2-4f91-b4f3-4d8c2b0a9580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 3: Holdout Split ===\n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X_full, y_full, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122e4b8f-9c56-4a42-a8f7-27a87c084755",
   "metadata": {},
   "source": [
    "## RMSE Function\n",
    "\n",
    "A custom function is defined to calculate RMSE (Root Mean Squared Error). It measures the average magnitude of prediction error. This function is wrapped with `make_scorer` to be compatible with scikit-learn’s model selection tools like GridSearchCV, allowing RMSE to be used during hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91e748b3-17d5-426a-aaa0-a3ad40612a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 4: RMSE Function ===\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "rmse_scorer = make_scorer(rmse, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196f2814-bc8f-4e6b-b24a-65338d1bbc7e",
   "metadata": {},
   "source": [
    "## Model Definitions\n",
    "\n",
    "A dictionary named `models` is created to store all the machine learning models that will be evaluated. This includes a mix of linear, tree-based, distance-based, kernel-based, and neural models. \n",
    "\n",
    "It covers: `Linear Regression`, `k-Nearest Neighbors (KNN)`, `Decision Tree`, `Support Vector Regression (SVR)`, a `Neural Network (MLPRegressor)`, `Random Forest`, `Gradient Boosting`, `XGBoost`, and `LightGBM`. This broad range of models allows for comparison between simple and complex learners and helps identify which types perform best for predicting spending behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e7c5b13-dc1d-4889-b823-287fceffb13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 5: Model Definitions ===\n",
    "models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'KNN': KNeighborsRegressor(),\n",
    "    'DecisionTree': DecisionTreeRegressor(),\n",
    "    'SVM': SVR(),\n",
    "    'NeuralNet': MLPRegressor(max_iter=200, random_state=42),\n",
    "    'RandomForest': RandomForestRegressor(),\n",
    "    'GradientBoosting': GradientBoostingRegressor(),\n",
    "    'XGBoost': XGBRegressor(objective='reg:squarederror', verbosity=0),\n",
    "    'LightGBM': LGBMRegressor(force_col_wise=True, enable_categorical=False,verbose=-1)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5694f907-4d69-4851-b9b3-dbf3556437bd",
   "metadata": {},
   "source": [
    "## Hyperparamter Grid\n",
    "\n",
    "Alongside the models, a dictionary called `param_grids` is defined to specify the hyperparameters to tune for each model during cross-validation. \n",
    "\n",
    "- For all models, a range of values is specified for key parameters such as number of neighbors, tree depth, and regularization strength.\n",
    "- These grids ensure that each model is optimized fairly before performance is compared. The tuning process will be handled later via GridSearchCV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5794bee-50f8-4926-9cba-08a776fe09e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'LinearRegression': {},\n",
    "\n",
    "    'KNN': {\n",
    "        'n_neighbors': [3, 5, 7],\n",
    "        'weights': ['uniform', 'distance']\n",
    "    },\n",
    "\n",
    "    'DecisionTree': {\n",
    "        'max_depth': [5, 10, None],\n",
    "        'min_samples_split': [2, 5]\n",
    "    },\n",
    "\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        \"kernel\": ['linear', 'rbf', 'poly']\n",
    "    },\n",
    "\n",
    "    'NeuralNet': {\n",
    "        'hidden_layer_sizes': [(32,), (64,), (64, 32)],\n",
    "        'alpha': [0.0001, 0.001, 0.01],\n",
    "        'learning_rate_init': [0.0005, 0.001],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'solver': ['adam']\n",
    "    },\n",
    "\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, None],\n",
    "        'max_features': ['sqrt'],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    },\n",
    "\n",
    "    'GradientBoosting': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'max_depth': [3, 5],\n",
    "        'subsample': [0.8],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    },\n",
    "\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'max_depth': [3, 5],\n",
    "        'subsample': [0.8],\n",
    "        'colsample_bytree': [0.8],\n",
    "        'reg_alpha': [0],\n",
    "        'reg_lambda': [1]\n",
    "    },\n",
    "\n",
    "    'LightGBM': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'max_depth': [5, 10],\n",
    "        'num_leaves': [31, 64],\n",
    "        'subsample': [0.8],\n",
    "        'colsample_bytree': [0.8],\n",
    "        'reg_alpha': [0],\n",
    "        'reg_lambda': [1]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9676019-9f45-46ff-b47d-84bf00404baa",
   "metadata": {},
   "source": [
    "## Nested Cross-Validation\n",
    "\n",
    "- This block implements a nested cross-validation loop to evaluate and tune each model using RMSE as the performance metric. The outer loop (5-fold) is used for evaluating generalization error, while the inner loop (3-fold) is used for hyperparameter tuning via `GridSearchCV`.\n",
    "\n",
    "- For each model in the dictionary, the data is split into training and validation sets within the outer loop. Before training, the numerical features are  standardized using `StandardScaler` — the scaling is applied only on the training data and then transformed onto the validation set to prevent data leakage.\n",
    "\n",
    "- Within each outer fold, a grid search is performed to find the best hyperparameters using the inner cross-validation loop. The best model from the inner CV is then used to predict on the outer fold’s validation set. The RMSE is computed and stored for that fold.\n",
    "\n",
    "- After all folds are completed, the average RMSE and its standard deviation across outer folds are computed for each model. This setup ensures fair and unbiased model comparison, with preprocessing handled carefully outside the pipeline to give full control over the transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fa9250a-8c29-4e79-bec0-1476a1cfdca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols  = [\"Freq\", \"last_update_days_ago\", \"1st_update_days_ago\"]\n",
    "binary_cols = [col for col in X_train.columns if col not in numeric_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "251b80f1-2511-475f-aaa7-a05087a6fd75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Freq', 'last_update_days_ago', '1st_update_days_ago']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c10c3d40-a23b-42df-8091-d35d075d1833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['US',\n",
       " 'source_a',\n",
       " 'source_c',\n",
       " 'source_b',\n",
       " 'source_d',\n",
       " 'source_e',\n",
       " 'source_m',\n",
       " 'source_o',\n",
       " 'source_h',\n",
       " 'source_r',\n",
       " 'source_s',\n",
       " 'source_t',\n",
       " 'source_u',\n",
       " 'source_p',\n",
       " 'source_x',\n",
       " 'source_w',\n",
       " 'Web order',\n",
       " 'Gender=male',\n",
       " 'Address_is_res']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10841ce6-070b-4cc5-852d-14545c13e723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LinearRegression...\n",
      "LinearRegression: Mean RMSE = 126.1608, Std = 15.9257\n",
      "\n",
      "Training KNN...\n",
      "KNN: Mean RMSE = 133.6503, Std = 20.4117\n",
      "\n",
      "Training DecisionTree...\n",
      "DecisionTree: Mean RMSE = 148.9928, Std = 22.7721\n",
      "\n",
      "Training SVM...\n",
      "SVM: Mean RMSE = 134.6211, Std = 15.0443\n",
      "\n",
      "Training NeuralNet...\n",
      "NeuralNet: Mean RMSE = 122.4092, Std = 16.8263\n",
      "\n",
      "Training RandomForest...\n",
      "RandomForest: Mean RMSE = 130.2882, Std = 17.2483\n",
      "\n",
      "Training GradientBoosting...\n",
      "GradientBoosting: Mean RMSE = 121.5868, Std = 20.3617\n",
      "\n",
      "Training XGBoost...\n",
      "XGBoost: Mean RMSE = 121.0807, Std = 19.8520\n",
      "\n",
      "Training LightGBM...\n",
      "LightGBM: Mean RMSE = 129.1533, Std = 18.6504\n"
     ]
    }
   ],
   "source": [
    "# === STEP 6: Nested CV ===\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "results = {}\n",
    "best_models = {}\n",
    "\n",
    "for name in models.keys():\n",
    "    model = models[name]\n",
    "    param_grid = param_grids[name]\n",
    "    outer_scores = []\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "\n",
    "    for train_idx, val_idx in outer_cv.split(X_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        preprocessor = ColumnTransformer([\n",
    "            (\"scale_numeric\", StandardScaler(), numeric_cols),\n",
    "            (\"passthrough_binary\", \"passthrough\", binary_cols)\n",
    "        ])\n",
    "    \n",
    "        X_tr_scaled = preprocessor.fit_transform(X_tr)\n",
    "        X_val_scaled = preprocessor.transform(X_val)\n",
    "\n",
    "        gs = GridSearchCV(model, param_grid, scoring=rmse_scorer, cv=inner_cv)\n",
    "        gs.fit(X_tr_scaled, y_tr)\n",
    "\n",
    "        best_model = gs.best_estimator_\n",
    "        y_pred = best_model.predict(X_val_scaled)\n",
    "        outer_rmse = rmse(y_val, y_pred)\n",
    "        outer_scores.append(outer_rmse)\n",
    "\n",
    "    mean_rmse = np.mean(outer_scores)\n",
    "    std_rmse = np.std(outer_scores)\n",
    "    results[name] = outer_scores\n",
    "\n",
    "    print(f\"{name}: Mean RMSE = {mean_rmse:.4f}, Std = {std_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa4df8a-e3bb-4b99-b801-0729ee170a63",
   "metadata": {},
   "source": [
    "## Final Model Comparison Summary\n",
    "\n",
    "Below is the summary of RMSE and standard deviation across all models from nested cross-validation:\n",
    "\n",
    "| Model              | RMSE     | Std Dev |\n",
    "|-------------------|----------|---------|\n",
    "| Linear Regression | 126.16   | 15.93   |\n",
    "| KNN               | 133.65   | 20.41   |\n",
    "| Decision Tree     | 148.99   | 22.77   |\n",
    "| SVM               | 134.62   | 15.04   |\n",
    "| Neural Net        | 122.41   | 16.83   |\n",
    "| Random Forest     | 130.29   | 17.25   |\n",
    "| Gradient Boosting | 121.59   | 20.36   |\n",
    "| XGBoost           | 121.08   | 19.85   |\n",
    "| LightGBM          | 129.15   | 18.65   |\n",
    "\n",
    "---\n",
    "\n",
    "### Results:\n",
    "\n",
    "- Among all models, **XGBoost** achieved the lowest RMSE of **121.08**, closely followed by **Gradient Boosting (121.59)** and **Neural Net (122.41)**. These models effectively capture complex non-linear patterns, leading to strong performance.\n",
    "\n",
    "- **Decision Tree** had the highest RMSE of **148.99**, along with a high standard deviation, indicating weak and unstable performance. **KNN** and **SVM** also underperformed, showing limitations in modeling capacity for this dataset.\n",
    "\n",
    "- Classical models such as **Linear Regression** showed reasonable performance, but were clearly outperformed by ensemble and neural models. **Random Forest** and **LightGBM** performed better but still fell short of the top-performing models.\n",
    "\n",
    "- **XGBoost** stands out as the most robust and accurate model across all folds. Based on its strong generalization performance, it is selected as the **final model** for further tuning and holdout evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63e07900-5538-4a9f-9207-f68a6866ac00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Nested CV RMSE Comparison:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LinearRegression</th>\n",
       "      <th>KNN</th>\n",
       "      <th>DecisionTree</th>\n",
       "      <th>SVM</th>\n",
       "      <th>NeuralNet</th>\n",
       "      <th>RandomForest</th>\n",
       "      <th>GradientBoosting</th>\n",
       "      <th>XGBoost</th>\n",
       "      <th>LightGBM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>126.16</td>\n",
       "      <td>133.65</td>\n",
       "      <td>148.99</td>\n",
       "      <td>134.62</td>\n",
       "      <td>122.41</td>\n",
       "      <td>130.29</td>\n",
       "      <td>121.59</td>\n",
       "      <td>121.08</td>\n",
       "      <td>129.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Std</th>\n",
       "      <td>15.93</td>\n",
       "      <td>20.41</td>\n",
       "      <td>22.77</td>\n",
       "      <td>15.04</td>\n",
       "      <td>16.83</td>\n",
       "      <td>17.25</td>\n",
       "      <td>20.36</td>\n",
       "      <td>19.85</td>\n",
       "      <td>18.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      LinearRegression     KNN  DecisionTree     SVM  NeuralNet  RandomForest  \\\n",
       "Mean            126.16  133.65        148.99  134.62     122.41        130.29   \n",
       "Std              15.93   20.41         22.77   15.04      16.83         17.25   \n",
       "\n",
       "      GradientBoosting  XGBoost  LightGBM  \n",
       "Mean            121.59   121.08    129.15  \n",
       "Std              20.36    19.85     18.65  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === STEP 7: Compare Results ===\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.loc['Mean'] = results_df.mean()\n",
    "results_df.loc['Std'] = results_df.std()\n",
    "print(\"\\nFinal Nested CV RMSE Comparison:\")\n",
    "results_df.round(2).tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cccd26-fca5-4819-bc0d-0a2b8c5ccc79",
   "metadata": {},
   "source": [
    "## Final XGBoost Model Run\n",
    "\n",
    "To finalize the model selection, we performed nested cross-validation using XGBoost with a carefully pruned hyperparameter grid. The outer loop was used to evaluate generalization performance, while the inner loop performed hyperparameter tuning via GridSearchCV. Preprocessing was handled using a ColumnTransformer, which scaled numeric features and passed binary variables as-is - ensuring no data leakage across folds.\n",
    "\n",
    "- The RMSEs across the five outer folds were:\n",
    "\n",
    "| Fold     | RMSE     |\n",
    "|----------|----------|\n",
    "| Fold 1   | 106.12   |\n",
    "| Fold 2   | 130.76   |\n",
    "| Fold 3   | 98.56    |\n",
    "| Fold 4   | 126.01   |\n",
    "| Fold 5   | 152.17   |\n",
    "\n",
    "- The average RMSE across these folds was **122.72**, with a standard deviation of **18.98**, indicating strong generalization with reasonable consistency across data splits.\n",
    "\n",
    "- After identifying the best-performing model (from Fold 3), we retrained it on the entire training set and evaluated it on the unseen holdout set. The final **Holdout RMSE was 128.43**, which aligns well with the cross-validation results.\n",
    "\n",
    "- This confirms that the XGBoost model maintains robust predictive performance and generalizes effectively to new, unseen customer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "027d9d0a-479d-44a0-82a1-fe8706187217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outer Fold 1/5\n",
      "Fold 1 RMSE: 106.1169\n",
      "\n",
      "Outer Fold 2/5\n",
      "Fold 2 RMSE: 130.7626\n",
      "\n",
      "Outer Fold 3/5\n",
      "Fold 3 RMSE: 98.5594\n",
      "\n",
      "Outer Fold 4/5\n",
      "Fold 4 RMSE: 126.0075\n",
      "\n",
      "Outer Fold 5/5\n",
      "Fold 5 RMSE: 152.1651\n",
      "\n",
      "Nested CV RMSE Results (XGBoost):\n",
      "Fold 1: 106.1169\n",
      "Fold 2: 130.7626\n",
      "Fold 3: 98.5594\n",
      "Fold 4: 126.0075\n",
      "Fold 5: 152.1651\n",
      "Mean RMSE: 122.7223\n",
      "Std Dev:   18.9837\n",
      "\n",
      "Best outer fold model: Fold 3 with RMSE = 98.5594\n",
      "Best Hyperparameters:\n",
      "  objective: reg:squarederror\n",
      "  base_score: None\n",
      "  booster: None\n",
      "  callbacks: None\n",
      "  colsample_bylevel: None\n",
      "  colsample_bynode: None\n",
      "  colsample_bytree: 0.8\n",
      "  device: None\n",
      "  early_stopping_rounds: None\n",
      "  enable_categorical: False\n",
      "  eval_metric: None\n",
      "  feature_types: None\n",
      "  feature_weights: None\n",
      "  gamma: 0\n",
      "  grow_policy: None\n",
      "  importance_type: None\n",
      "  interaction_constraints: None\n",
      "  learning_rate: 0.05\n",
      "  max_bin: None\n",
      "  max_cat_threshold: None\n",
      "  max_cat_to_onehot: None\n",
      "  max_delta_step: None\n",
      "  max_depth: 3\n",
      "  max_leaves: None\n",
      "  min_child_weight: 1\n",
      "  missing: nan\n",
      "  monotone_constraints: None\n",
      "  multi_strategy: None\n",
      "  n_estimators: 200\n",
      "  n_jobs: None\n",
      "  num_parallel_tree: None\n",
      "  random_state: 42\n",
      "  reg_alpha: 0.1\n",
      "  reg_lambda: 1\n",
      "  sampling_method: None\n",
      "  scale_pos_weight: None\n",
      "  subsample: 0.8\n",
      "  tree_method: None\n",
      "  validate_parameters: None\n",
      "  verbosity: 0\n",
      "\n",
      "Holdout RMSE: 128.4273\n"
     ]
    }
   ],
   "source": [
    "# ======= RMSE Scorer =======\n",
    "rmse = lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "rmse_scorer = make_scorer(rmse, greater_is_better=False)\n",
    "\n",
    "# ======= XGBoost Grid =======\n",
    "xgb_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8],\n",
    "    'reg_alpha': [0, 0.1],\n",
    "    'reg_lambda': [1, 2],\n",
    "    'min_child_weight': [1, 3],\n",
    "    'gamma': [0, 0.1]\n",
    "}\n",
    "\n",
    "# ======= Nested CV Setup =======\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "outer_scores = []\n",
    "best_models = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(outer_cv.split(X_train)):\n",
    "    print(f\"\\nOuter Fold {fold+1}/5\")\n",
    "\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"scale_numeric\", StandardScaler(), numeric_cols),\n",
    "        (\"passthrough_binary\", \"passthrough\", binary_cols)\n",
    "    ])\n",
    "    \n",
    "    X_tr_scaled = preprocessor.fit_transform(X_tr)\n",
    "    X_val_scaled = preprocessor.transform(X_val)\n",
    "\n",
    "    xgb = XGBRegressor(objective=\"reg:squarederror\", verbosity=0, random_state=42)\n",
    "    grid = GridSearchCV(\n",
    "        xgb,\n",
    "        xgb_grid,\n",
    "        scoring=rmse_scorer,\n",
    "        cv=inner_cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    grid.fit(X_tr_scaled, y_tr)\n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "    y_val_pred = best_model.predict(X_val_scaled)\n",
    "    val_rmse = rmse(y_val, y_val_pred)\n",
    "\n",
    "    best_models.append((best_model, preprocessor, val_rmse))\n",
    "    outer_scores.append(val_rmse)\n",
    "\n",
    "    print(f\"Fold {fold+1} RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "# ======= Nested CV Summary =======\n",
    "mean_rmse = np.mean(outer_scores)\n",
    "std_rmse = np.std(outer_scores)\n",
    "\n",
    "print(\"\\nNested CV RMSE Results (XGBoost):\")\n",
    "for i, score in enumerate(outer_scores):\n",
    "    print(f\"Fold {i+1}: {score:.4f}\")\n",
    "print(f\"Mean RMSE: {mean_rmse:.4f}\")\n",
    "print(f\"Std Dev:   {std_rmse:.4f}\")\n",
    "\n",
    "# ======= Select Best Model from Outer CV =======\n",
    "best_index = np.argmin(outer_scores)\n",
    "best_estimator, best_scaler, best_score = best_models[best_index]\n",
    "print(f\"\\nBest outer fold model: Fold {best_index+1} with RMSE = {best_score:.4f}\")\n",
    "print(\"Best Hyperparameters:\")\n",
    "for param, value in best_estimator.get_params().items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "    \n",
    "# ======= Refit on Full Training Set =======\n",
    "preprocessor = ColumnTransformer([\n",
    "        (\"scale_numeric\", StandardScaler(), numeric_cols),\n",
    "        (\"passthrough_binary\", \"passthrough\", binary_cols)\n",
    "    ])\n",
    "X_train_scaled = preprocessor.fit_transform(X_train)\n",
    "X_holdout_scaled = preprocessor.transform(X_holdout)\n",
    "\n",
    "final_model = best_estimator.set_params(**best_estimator.get_params())\n",
    "final_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# ======= Holdout Evaluation =======\n",
    "y_holdout_pred = final_model.predict(X_holdout_scaled)\n",
    "holdout_rmse = rmse(y_holdout, y_holdout_pred)\n",
    "print(f\"\\nHoldout RMSE: {holdout_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6364e17d-9b5b-4bb0-a910-8ca10f137027",
   "metadata": {},
   "source": [
    "# Part B - Modeling Only for Purchasers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406d5ff2-3d52-458b-9536-b4df9acbf719",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "The dataset `hw3data.csv` is loaded into a pandas DataFrame. Here, we filtered for customers who had `Purchase = 1` for our modeling. From this, three columns — `sequence_number`, `Spending`, and `Purchase` — are removed to isolate the predictors into `X_full`. The target variable `y_full` is defined separately using the `Spending` column. This setup ensures that only relevant features are used for training, while the target is clearly separated for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "809bf3d4-067a-4a0b-90a2-23a51a3f42be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 2: Load Data ===\n",
    "df = pd.read_csv(\"hw3data.csv\")\n",
    "df = df[df['Purchase']==1]\n",
    "X_full = df.drop(columns=[\"sequence_number\", \"Spending\", \"Purchase\"])\n",
    "y_full = df[\"Spending\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8d7d06-496f-4a9d-815e-2ec1cd846382",
   "metadata": {},
   "source": [
    "## Holdout Split\n",
    "\n",
    "The dataset is then split into training and holdout sets using an 80-20 split. This means 80% of the data will be used to train and validate models through cross-validation, while 20% will be reserved as an untouched set to evaluate final model performance. A fixed random seed ensures the results are reproducible across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c2093bc4-5387-4f18-8fd5-5f5ab252c3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 3: Holdout Split ===\n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X_full, y_full, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ca70a7-b4a4-48cd-804d-2683155c3481",
   "metadata": {},
   "source": [
    "## RMSE Function\n",
    "\n",
    "A custom function is defined to calculate RMSE (Root Mean Squared Error). It measures the average magnitude of prediction error. This function is wrapped with `make_scorer` to be compatible with scikit-learn’s model selection tools like GridSearchCV, allowing RMSE to be used during hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "11ee3f64-c0e5-48c2-b42c-249852e3644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 4: RMSE Function ===\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "rmse_scorer = make_scorer(rmse, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecb43af-cfe3-4a42-8700-f88ba8ca9b12",
   "metadata": {},
   "source": [
    "## Model Definitions\n",
    "\n",
    "A dictionary named `models` is created to store all the machine learning models that will be evaluated. This includes a mix of linear, tree-based, distance-based, kernel-based, and neural models. \n",
    "\n",
    "It covers: `Linear Regression`, `k-Nearest Neighbors (KNN)`, `Decision Tree`, `Support Vector Regression (SVR)`, a `Neural Network (MLPRegressor)`, `Random Forest`, `Gradient Boosting`, `XGBoost`, and `LightGBM`. This broad range of models allows for comparison between simple and complex learners and helps identify which types perform best for predicting spending behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "082ea641-f055-4153-bbaa-9f14b692bef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 5: Model Definitions ===\n",
    "models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'KNN': KNeighborsRegressor(),\n",
    "    'DecisionTree': DecisionTreeRegressor(),\n",
    "    'SVM': SVR(),\n",
    "    'NeuralNet': MLPRegressor(max_iter=200, random_state=42),\n",
    "    'RandomForest': RandomForestRegressor(),\n",
    "    'GradientBoosting': GradientBoostingRegressor(),\n",
    "    'XGBoost': XGBRegressor(objective='reg:squarederror', verbosity=0),\n",
    "    'LightGBM': LGBMRegressor(force_col_wise=True, enable_categorical=False,verbose=-1)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e0ffb-e4c9-4231-9fec-12ee362b7ab6",
   "metadata": {},
   "source": [
    "## Hyperparamter Grid\n",
    "\n",
    "Alongside the models, a dictionary called `param_grids` is defined to specify the hyperparameters to tune for each model during cross-validation. \n",
    "\n",
    "- For models like KNN, Decision Tree, and SVM, a range of values is specified for key parameters such as number of neighbors, tree depth, and regularization strength.\n",
    "- These grids ensure that each model is optimized fairly before performance is compared. The tuning process will be handled later via GridSearchCV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5684128b-b374-4584-b82d-d9f3ce7d98a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'LinearRegression': {},\n",
    "\n",
    "    'KNN': {\n",
    "        'n_neighbors': [3, 5, 7],\n",
    "        'weights': ['uniform', 'distance']\n",
    "    },\n",
    "\n",
    "    'DecisionTree': {\n",
    "        'max_depth': [5, 10, None],\n",
    "        'min_samples_split': [2, 5]\n",
    "    },\n",
    "\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        \"kernel\": ['linear', 'rbf', 'poly']\n",
    "    },\n",
    "\n",
    "    'NeuralNet': {\n",
    "        'hidden_layer_sizes': [(32,), (64,), (64, 32)],\n",
    "        'alpha': [0.0001, 0.001, 0.01],\n",
    "        'learning_rate_init': [0.0005, 0.001],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'solver': ['adam']\n",
    "    },\n",
    "\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, None],\n",
    "        'max_features': ['sqrt'],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    },\n",
    "\n",
    "    'GradientBoosting': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'max_depth': [3, 5],\n",
    "        'subsample': [0.8],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    },\n",
    "\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'max_depth': [3, 5],\n",
    "        'subsample': [0.8],\n",
    "        'colsample_bytree': [0.8],\n",
    "        'reg_alpha': [0],\n",
    "        'reg_lambda': [1]\n",
    "    },\n",
    "\n",
    "    'LightGBM': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'max_depth': [5, 10],\n",
    "        'num_leaves': [31, 64],\n",
    "        'subsample': [0.8],\n",
    "        'colsample_bytree': [0.8],\n",
    "        'reg_alpha': [0],\n",
    "        'reg_lambda': [1]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7d4928-0610-4a98-866c-b3c16430fbbe",
   "metadata": {},
   "source": [
    "## Nested Cross-Validation\n",
    "\n",
    "- This block implements a nested cross-validation loop to evaluate and tune each model using RMSE as the performance metric. The outer loop (5-fold) is used for evaluating generalization error, while the inner loop (3-fold) is used for hyperparameter tuning via `GridSearchCV`.\n",
    "\n",
    "- For each model in the dictionary, the data is split into training and validation sets within the outer loop. Before training, the numerical features are  standardized using `StandardScaler` — the scaling is applied only on the training data and then transformed onto the validation set to prevent data leakage.\n",
    "\n",
    "- Within each outer fold, a grid search is performed to find the best hyperparameters using the inner cross-validation loop. The best model from the inner CV is then used to predict on the outer fold’s validation set. The RMSE is computed and stored for that fold.\n",
    "\n",
    "- After all folds are completed, the average RMSE and its standard deviation across outer folds are computed for each model. This setup ensures fair and unbiased model comparison, with preprocessing handled carefully outside the pipeline to give full control over the transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "55d5b0fb-5761-48a8-9da6-797e11b6720f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LinearRegression...\n",
      "LinearRegression: Mean RMSE = 157.8116, Std = 20.8627\n",
      "\n",
      "Training KNN...\n",
      "KNN: Mean RMSE = 162.6334, Std = 20.0380\n",
      "\n",
      "Training DecisionTree...\n",
      "DecisionTree: Mean RMSE = 184.6255, Std = 13.0663\n",
      "\n",
      "Training SVM...\n",
      "SVM: Mean RMSE = 166.6539, Std = 23.2166\n",
      "\n",
      "Training NeuralNet...\n",
      "NeuralNet: Mean RMSE = 154.6944, Std = 19.9925\n",
      "\n",
      "Training RandomForest...\n",
      "RandomForest: Mean RMSE = 157.9787, Std = 22.5538\n",
      "\n",
      "Training GradientBoosting...\n",
      "GradientBoosting: Mean RMSE = 155.0353, Std = 19.2456\n",
      "\n",
      "Training XGBoost...\n",
      "XGBoost: Mean RMSE = 153.8904, Std = 19.2866\n",
      "\n",
      "Training LightGBM...\n",
      "LightGBM: Mean RMSE = 161.0574, Std = 23.2587\n"
     ]
    }
   ],
   "source": [
    "# === STEP 6: Nested CV ===\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "results = {}\n",
    "best_models = {}\n",
    "\n",
    "for name in models.keys():\n",
    "    model = models[name]\n",
    "    param_grid = param_grids[name]\n",
    "    outer_scores = []\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "\n",
    "    for train_idx, val_idx in outer_cv.split(X_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        preprocessor = ColumnTransformer([\n",
    "            (\"scale_numeric\", StandardScaler(), numeric_cols),\n",
    "            (\"passthrough_binary\", \"passthrough\", binary_cols)\n",
    "        ])\n",
    "\n",
    "        X_tr_scaled = preprocessor.fit_transform(X_tr)\n",
    "        X_val_scaled = preprocessor.transform(X_val)\n",
    "\n",
    "        gs = GridSearchCV(\n",
    "            model,\n",
    "            param_grid,\n",
    "            scoring=rmse_scorer,\n",
    "            cv=inner_cv\n",
    "        )\n",
    "        gs.fit(X_tr_scaled, y_tr)\n",
    "\n",
    "        best_model = gs.best_estimator_\n",
    "        y_pred = best_model.predict(X_val_scaled)\n",
    "        outer_rmse = rmse(y_val, y_pred)\n",
    "        outer_scores.append(outer_rmse)\n",
    "\n",
    "    mean_rmse = np.mean(outer_scores)\n",
    "    std_rmse = np.std(outer_scores)\n",
    "    results[name] = outer_scores\n",
    "    best_models[name] = best_model\n",
    "\n",
    "    print(f\"{name}: Mean RMSE = {mean_rmse:.4f}, Std = {std_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ffa399-4337-42ba-8e37-bcc83422974f",
   "metadata": {},
   "source": [
    "## Final Model Comparison Summary\n",
    "\n",
    "Below is the summary of RMSE and standard deviation across all models from nested cross-validation:\n",
    "\n",
    "| Model             | RMSE     | Std Dev |\n",
    "|------------------|----------|---------|\n",
    "| Linear Regression| 157.81   | 20.86   |\n",
    "| KNN              | 162.63   | 20.04   |\n",
    "| Decision Tree    | 184.63   | 13.07   |\n",
    "| SVM              | 166.65   | 23.22   |\n",
    "| Neural Net       | 154.69   | 19.99   |\n",
    "| Random Forest    | 157.98   | 22.55   |\n",
    "| Gradient Boosting| 155.04   | 19.25   |\n",
    "| XGBoost          | 153.89   | 19.29   |\n",
    "| LightGBM         | 161.06   | 23.26   |\n",
    "\n",
    "---\n",
    "\n",
    "### Results:\n",
    "\n",
    "- **XGBoost** achieved the best performance with the lowest RMSE of **153.89**, closely followed by **Neural Net (154.69)** and **Gradient Boosting (155.04)**. All three performed strongly, thanks to their ability to model complex non-linear relationships.\n",
    "\n",
    "- **Decision Tree** performed the worst (RMSE = 184.63), with a relatively low standard deviation — consistently underperforming across folds.\n",
    "\n",
    "- **SVM** and **KNN** had relatively high RMSEs, suggesting they may not be ideal for this particular regression task.\n",
    "\n",
    "- Traditional models like **Linear Regression** and **Random Forest** offered reasonable performance, but were outperformed by boosting and neural architectures.\n",
    "\n",
    "- Based on the trade-off between performance and consistency, **XGBoost** can be selected as the final model for retraining and holdout evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2b3f6f23-e03f-4adb-b5ae-b0fa7d75e41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Nested CV RMSE Comparison:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LinearRegression</th>\n",
       "      <th>KNN</th>\n",
       "      <th>DecisionTree</th>\n",
       "      <th>SVM</th>\n",
       "      <th>NeuralNet</th>\n",
       "      <th>RandomForest</th>\n",
       "      <th>GradientBoosting</th>\n",
       "      <th>XGBoost</th>\n",
       "      <th>LightGBM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>157.81</td>\n",
       "      <td>162.63</td>\n",
       "      <td>184.63</td>\n",
       "      <td>166.65</td>\n",
       "      <td>154.69</td>\n",
       "      <td>157.98</td>\n",
       "      <td>155.04</td>\n",
       "      <td>153.89</td>\n",
       "      <td>161.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Std</th>\n",
       "      <td>20.86</td>\n",
       "      <td>20.04</td>\n",
       "      <td>13.07</td>\n",
       "      <td>23.22</td>\n",
       "      <td>19.99</td>\n",
       "      <td>22.55</td>\n",
       "      <td>19.25</td>\n",
       "      <td>19.29</td>\n",
       "      <td>23.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      LinearRegression     KNN  DecisionTree     SVM  NeuralNet  RandomForest  \\\n",
       "Mean            157.81  162.63        184.63  166.65     154.69        157.98   \n",
       "Std              20.86   20.04         13.07   23.22      19.99         22.55   \n",
       "\n",
       "      GradientBoosting  XGBoost  LightGBM  \n",
       "Mean            155.04   153.89    161.06  \n",
       "Std              19.25    19.29     23.26  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === STEP 7: Compare Results ===\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.loc['Mean'] = results_df.mean()\n",
    "results_df.loc['Std'] = results_df.std()\n",
    "print(\"\\nFinal Nested CV RMSE Comparison:\")\n",
    "results_df.round(2).tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429dff13-08f3-4972-9c1d-e24838a2c289",
   "metadata": {},
   "source": [
    "## Final XGBoost Model Run\n",
    "\n",
    "To finalize the model selection, we performed nested cross-validation using XGBoost with a carefully defined hyperparameter grid. The outer loop was used to evaluate generalization performance, while the inner loop performed hyperparameter tuning via GridSearchCV. ColumnTransformer with `StandardScaler` was applied only on the numeric features within each fold to prevent data leakage.\n",
    "\n",
    "- The RMSEs across the five outer folds were:\n",
    "\n",
    "| Fold     | RMSE     |\n",
    "|----------|----------|\n",
    "| Fold 1   | 191.1722 |\n",
    "| Fold 2   | 147.4471 |\n",
    "| Fold 3   | 145.6280 |\n",
    "| Fold 4   | 151.0174 |\n",
    "| Fold 5   | 139.3723 |\n",
    "\n",
    "---\n",
    "\n",
    "## Results:\n",
    "- The average RMSE across these folds was **154.93**, with a standard deviation of **18.51**, indicating stable performance with modest variation across folds.\n",
    "\n",
    "- After identifying the best-performing model (from Fold 5), we retrained it on the entire training set and evaluated it on the unseen holdout set. The final **Holdout RMSE was 185.87**, which, while slightly higher than the cross-validation average, remains within a reasonable range.\n",
    "\n",
    "- These results confirm that the **XGBoost model** is a strong candidate for this regression task, showing competitive performance both in validation and on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b144f360-7274-4a94-8029-d09ef85506a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outer Fold 1/5\n",
      "Fold 1 RMSE: 191.1722\n",
      "\n",
      "Outer Fold 2/5\n",
      "Fold 2 RMSE: 147.4471\n",
      "\n",
      "Outer Fold 3/5\n",
      "Fold 3 RMSE: 145.6280\n",
      "\n",
      "Outer Fold 4/5\n",
      "Fold 4 RMSE: 151.0174\n",
      "\n",
      "Outer Fold 5/5\n",
      "Fold 5 RMSE: 139.3723\n",
      "\n",
      "Nested CV RMSE Results (XGBoost):\n",
      "Fold 1: 191.1722\n",
      "Fold 2: 147.4471\n",
      "Fold 3: 145.6280\n",
      "Fold 4: 151.0174\n",
      "Fold 5: 139.3723\n",
      "Mean RMSE: 154.9274\n",
      "Std Dev:   18.5115\n",
      "\n",
      "Best outer fold model: Fold 5 with RMSE = 139.3723\n",
      "Best Hyperparameters:\n",
      "  objective: reg:squarederror\n",
      "  base_score: None\n",
      "  booster: None\n",
      "  callbacks: None\n",
      "  colsample_bylevel: None\n",
      "  colsample_bynode: None\n",
      "  colsample_bytree: 0.8\n",
      "  device: None\n",
      "  early_stopping_rounds: None\n",
      "  enable_categorical: False\n",
      "  eval_metric: None\n",
      "  feature_types: None\n",
      "  feature_weights: None\n",
      "  gamma: 0\n",
      "  grow_policy: None\n",
      "  importance_type: None\n",
      "  interaction_constraints: None\n",
      "  learning_rate: 0.05\n",
      "  max_bin: None\n",
      "  max_cat_threshold: None\n",
      "  max_cat_to_onehot: None\n",
      "  max_delta_step: None\n",
      "  max_depth: 3\n",
      "  max_leaves: None\n",
      "  min_child_weight: 3\n",
      "  missing: nan\n",
      "  monotone_constraints: None\n",
      "  multi_strategy: None\n",
      "  n_estimators: 100\n",
      "  n_jobs: None\n",
      "  num_parallel_tree: None\n",
      "  random_state: 42\n",
      "  reg_alpha: 0\n",
      "  reg_lambda: 2\n",
      "  sampling_method: None\n",
      "  scale_pos_weight: None\n",
      "  subsample: 0.8\n",
      "  tree_method: None\n",
      "  validate_parameters: None\n",
      "  verbosity: 0\n",
      "\n",
      "Holdout RMSE: 185.8695\n"
     ]
    }
   ],
   "source": [
    "# ======= RMSE Scorer =======\n",
    "rmse = lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "rmse_scorer = make_scorer(rmse, greater_is_better=False)\n",
    "\n",
    "# ======= XGBoost Grid =======\n",
    "xgb_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8],\n",
    "    'reg_alpha': [0, 0.1],\n",
    "    'reg_lambda': [1, 2],\n",
    "    'min_child_weight': [1, 3],\n",
    "    'gamma': [0, 0.1]\n",
    "}\n",
    "\n",
    "# ======= Nested CV Setup =======\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "outer_scores = []\n",
    "best_models = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(outer_cv.split(X_train)):\n",
    "    print(f\"\\nOuter Fold {fold+1}/5\")\n",
    "\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"scale_numeric\", StandardScaler(), numeric_cols),\n",
    "        (\"passthrough_binary\", \"passthrough\", binary_cols)\n",
    "    ])\n",
    "    \n",
    "    X_tr_scaled = preprocessor.fit_transform(X_tr)\n",
    "    X_val_scaled = preprocessor.transform(X_val)\n",
    "\n",
    "    xgb = XGBRegressor(objective=\"reg:squarederror\", verbosity=0, random_state=42)\n",
    "    grid = GridSearchCV(\n",
    "        xgb,\n",
    "        xgb_grid,\n",
    "        scoring=rmse_scorer,\n",
    "        cv=inner_cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    grid.fit(X_tr_scaled, y_tr)\n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "    y_val_pred = best_model.predict(X_val_scaled)\n",
    "    val_rmse = rmse(y_val, y_val_pred)\n",
    "\n",
    "    best_models.append((best_model, preprocessor, val_rmse))\n",
    "    outer_scores.append(val_rmse)\n",
    "\n",
    "    print(f\"Fold {fold+1} RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "# ======= Nested CV Summary =======\n",
    "mean_rmse = np.mean(outer_scores)\n",
    "std_rmse = np.std(outer_scores)\n",
    "\n",
    "print(\"\\nNested CV RMSE Results (XGBoost):\")\n",
    "for i, score in enumerate(outer_scores):\n",
    "    print(f\"Fold {i+1}: {score:.4f}\")\n",
    "print(f\"Mean RMSE: {mean_rmse:.4f}\")\n",
    "print(f\"Std Dev:   {std_rmse:.4f}\")\n",
    "\n",
    "# ======= Select Best Model from Outer CV =======\n",
    "best_index = np.argmin(outer_scores)\n",
    "best_estimator, best_scaler, best_score = best_models[best_index]\n",
    "print(f\"\\nBest outer fold model: Fold {best_index+1} with RMSE = {best_score:.4f}\")\n",
    "print(\"Best Hyperparameters:\")\n",
    "for param, value in best_estimator.get_params().items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "    \n",
    "# ======= Refit on Full Training Set =======\n",
    "preprocessor = ColumnTransformer([\n",
    "        (\"scale_numeric\", StandardScaler(), numeric_cols),\n",
    "        (\"passthrough_binary\", \"passthrough\", binary_cols)\n",
    "    ])\n",
    "X_train_scaled = preprocessor.fit_transform(X_train)\n",
    "X_holdout_scaled = preprocessor.transform(X_holdout)\n",
    "\n",
    "final_model = best_estimator.set_params(**best_estimator.get_params())\n",
    "final_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# ======= Holdout Evaluation =======\n",
    "y_holdout_pred = final_model.predict(X_holdout_scaled)\n",
    "holdout_rmse = rmse(y_holdout, y_holdout_pred)\n",
    "print(f\"\\nHoldout RMSE: {holdout_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1bba0c-e2de-49e4-b320-69731d815add",
   "metadata": {},
   "source": [
    "# Part C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c59c273-0fce-43a1-99e3-444080b107a1",
   "metadata": {},
   "source": [
    "### Statistical Comparison: Dependent Variable `Spending` in Task (a) vs. Task (b)\n",
    "\n",
    "| Metric      | Task (a): All Data | Task (b): Purchase = 1 Only | Difference       |\n",
    "|-------------|--------------------|------------------------------|------------------|\n",
    "| **Mean**    | 102.56             | 205.09                       | **+102.53 (doubled)** |\n",
    "| **Std Dev** | 186.75             | 220.77                       | **+34.02**     |\n",
    "| **Variance**| 34,875.49          | 48,740.12                    | **+13,864.63** |\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- The **standard deviation and variance** are significantly higher in Task (b)\n",
    "- This proves that:\n",
    "  > Predicting `Spending` becomes **more variable and challenging** when the dataset is restricted to actual purchasers\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "When we remove the many \"non-purchase\" records from the dataset (i.e., `Purchase = 0`), the task becomes much more difficult for predictive models. This is reflected in the **increased standard deviation and variance** of the target variable `Spending`. In the restricted dataset, consumers spend a **wider and more unpredictable** range of amounts, increasing noise and reducing model accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2e7505a-66c8-43c9-b689-383df47b4fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Full Dataset (Task a) ===\n",
      "Mean:     102.56\n",
      "Std Dev:  186.75\n",
      "Variance: 34875.49\n",
      "\n",
      "=== Restricted Dataset (Task b) ===\n",
      "Mean:     205.09\n",
      "Std Dev:  220.77\n",
      "Variance: 48740.12\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(\"hw3data.csv\")\n",
    "\n",
    "# Full dataset stats\n",
    "spending_all = data[\"Spending\"]\n",
    "mean_all = spending_all.mean()\n",
    "std_all = spending_all.std()\n",
    "var_all = spending_all.var()\n",
    "\n",
    "# Restricted dataset stats (Purchase = 1)\n",
    "spending_restricted = df[df[\"Purchase\"] == 1][\"Spending\"]\n",
    "mean_restricted = spending_restricted.mean()\n",
    "std_restricted = spending_restricted.std()\n",
    "var_restricted = spending_restricted.var()\n",
    "\n",
    "# Display results\n",
    "print(\"=== Full Dataset (Task a) ===\")\n",
    "print(f\"Mean:     {mean_all:.2f}\")\n",
    "print(f\"Std Dev:  {std_all:.2f}\")\n",
    "print(f\"Variance: {var_all:.2f}\")\n",
    "\n",
    "print(\"\\n=== Restricted Dataset (Task b) ===\")\n",
    "print(f\"Mean:     {mean_restricted:.2f}\")\n",
    "print(f\"Std Dev:  {std_restricted:.2f}\")\n",
    "print(f\"Variance: {var_restricted:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3f074c-2738-4808-a019-657d7f7e8fc1",
   "metadata": {},
   "source": [
    "## Final Takeaways by Model\n",
    "\n",
    "| Model                  | Insight                                                                                         |\n",
    "|------------------------|-------------------------------------------------------------------------------------------------|\n",
    "| **XGBoost**          | Good performer in both tasks - handles complexity and generalizes well even when 0s are removed |\n",
    "| **NeuralNet**            | Still strong, but performance edge reduced when predicting true spend values only               |\n",
    "| **GradientBoosting / RF** | Reliable but less dominant without 0-heavy patterns                                          |\n",
    "| **LinearRegression**   | Performed well on all data, but weak on actual purchasers due to inability to capture complex spending patterns |\n",
    "| **SVM / KNN / Trees**  | Sensitive to noise and lack of simple patterns - struggled more in Task (b)                     |\n",
    "\n",
    "### Why All Models Perform Worse in Task (b)\n",
    "\n",
    "- **Loss of Easy-to-Predict \"0s\"**  \n",
    "  In Task (a), many consumers had `Spending = 0`. Most models learned to predict **zero**, which lowered RMSE.\n",
    "\n",
    "- **Higher Variability in Task (b)**  \n",
    "  Among `Purchase = 1`, the `Spending` values are spread out → more noise, harder regression.\n",
    "\n",
    "- **Smaller Dataset in Task (b)**  \n",
    "  We are training on fewer rows, so models are more likely to overfit or underperform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8897dca-fa60-441b-8e0b-62e350fab898",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
